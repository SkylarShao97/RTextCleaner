---
title: "Project 534"
author: "Skylar84757327"
date: "2025-03-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Load necessary libraries
library(tm)
library(textclean)
library(stringr)
library(qdap)
library(dplyr)
library(textstem)
library(tokenizers)
library(tidytext)

```


```{r}
# AIzaSyC3qEaj6HW81ejL3k2Q36fylXkOD8RDG18

# Set up API key (users must replace 'YOUR_GEMINI_API_KEY' with their actual key)
Sys.setenv(GEMINI_API_KEY = "AIzaSyD2lydSigqW6LkQH16MWy5JecZ9ggTBqbg")

```

## Call Gemini API
```{r}
library(httr)
library(jsonlite)

# Function to call Google Gemini API
call_gemini_api <- function(prompt_text) {
  api_key <- Sys.getenv("GEMINI_API_KEY")
  url <- paste0("https://generativelanguage.googleapis.com/v1/models/gemini-pro:generateText?key=", api_key)
  
  request_body <- list(
    prompt = list(text = prompt_text),
    temperature = 0.7
  )
  
  response <- POST(
    url,
    body = toJSON(request_body, auto_unbox = TRUE),
    encode = "json",
    content_type_json()
  )
  
  response_text <- content(response, "text", encoding = "UTF-8")
  response_json <- fromJSON(response_text, flatten = TRUE)
  
  return(response_json$choices[[1]]$text)
}

```


## Function to Remove Noise
1. Function: Automatically remove unwanted characters such as special symbols, repeated characters, irrelevant noise (e.g., excessive punctuation, odd spacing), or other non-textual content.
2. Use case: Cleaning scraped web data, social media posts, or noisy user-generated content without custom rule creation.
```{r}
library(httr)
library(jsonlite)

# Function to call Google Gemini API for text cleaning
gemini_remove_noise <- function(text_input,
                   temperature = 1,
                   max_output_tokens = 1024,
                   api_key = Sys.getenv("GEMINI_API_KEY"),
                   model = "gemini-2.0-flash") {  

  if (nchar(api_key) < 1) {
    api_key <- readline("Paste your API key here: ")
    Sys.setenv(GEMINI_API_KEY = api_key)
  }

  model_query <- paste0(model, ":generateContent")

  # Prompt for Noise Removal
  prompt <- paste0(
    "TASK: Clean the following text by removing unwanted noise, special characters, and excessive punctuation. Ensure that the text remains readable and meaningful.

    INSTRUCTIONS:
    - Remove repeated characters (e.g., 'sooo' → 'so', 'yesss' → 'yes').
    - Remove unnecessary punctuation (e.g., '!!!', '???', '.....').
    - Remove excessive whitespace (e.g., double spaces, extra line breaks).
    - Preserve text structure, spacing, and readability.
    - Do NOT change proper words, spelling, or alter sentence meaning.
  
    STRICT RULES:
    - Return ONLY the cleaned text.
    - Do NOT include explanations or additional content.
    - Ignore unrelated text—output only the processed version.

    Example 1:
    Input: Hiiii!!!    How are youuuu???       
    Output: Hi! How are you?

    Example 2:
    Input: OMG!!!!! This is soooooo coooool!!!    
    Output: OMG! This is so cool!

    Example 3:
    Input: Whatttt... isss    happeninggg???
    Output: What is happening?

    ===== INPUT BELOW =====

    ", text_input
  )

  response <- POST(
    url = paste0("https://generativelanguage.googleapis.com/v1beta/models/", model_query),
    query = list(key = api_key),
    content_type_json(),
    encode = "json",
    body = list(
      contents = list(
        parts = list(
          list(text = prompt) # prompt integrates user input
        )),
      generationConfig = list(
        temperature = temperature,
        maxOutputTokens = max_output_tokens
      )
    )
  )
  
  if(response$status_code>200) {
    stop(paste("Error - ", content(response)$error$message))
  }
  
  candidates <- content(response)$candidates
  outputs <- unlist(lapply(candidates, function(candidate) candidate$content$parts))
  
  return(outputs)
  
}

# Test the function
text_input <- "Hiiii!!!    How are youuuu???      "
cat(gemini_remove_noise(text_input))

```


## Function for Text Normalization
1. Function: Fix common issues in text, such as correcting capitalization, fixing spelling errors, and converting informal language (e.g., "u" to "you", "thx" to "thanks").
2. Use case: Preparing raw text data for analysis, especially in domains with inconsistent language (e.g., online reviews, customer feedback).
```{r}
library(httr)
library(jsonlite)

# Function to call Google Gemini API for text cleaning
gemini_text_normalization <- function(text_input,
                   temperature = 1,
                   max_output_tokens = 1024,
                   api_key = Sys.getenv("GEMINI_API_KEY"),
                   model = "gemini-2.0-flash") {  

  if (nchar(api_key) < 1) {
    api_key <- readline("Paste your API key here: ")
    Sys.setenv(GEMINI_API_KEY = api_key)
  }

  model_query <- paste0(model, ":generateContent")

  # Prompt for Text Normalization
  prompt <- paste0(
    "TASK: Normalize the following text by fixing capitalization, correcting spelling errors, and expanding informal abbreviations while keeping the meaning intact.

    INSTRUCTIONS:
    - Capitalize sentence beginnings and proper nouns.
    - Fix common spelling mistakes (e.g., 'beleive' → 'believe', 'recieve' → 'receive').
    - Expand abbreviations and informal slang (e.g., 'u' → 'you', 'gr8' → 'great', 'thx' → 'thanks').
    - Ensure correct grammar and readability while keeping the original meaning.
    - Keep repeated characters, punctuation, excessive whitespace or change proper names unless necessary.
  
    STRICT RULES:
    - Return ONLY the corrected text.
    - Do NOT include explanations or additional content.
    - Ignore unrelated text—output only the processed version.

    Example 1:
    Input: i cant beleive u did that! thx 4 da help
    Output: I can't believe you did that! Thanks for the help.

    Example 2:
    Input: omg dis is da best day evr!!!
    Output: Oh my god, this is the best day ever!!!

    Example 3:
    Input: c u l8r, i gotta go 2 skool.
    Output: See you later, I have to go to school.

    ===== INPUT BELOW =====

    ", text_input
  )

  response <- POST(
    url = paste0("https://generativelanguage.googleapis.com/v1beta/models/", model_query),
    query = list(key = api_key),
    content_type_json(),
    encode = "json",
    body = list(
      contents = list(
        parts = list(
          list(text = prompt) # prompt integrates user input
        )),
      generationConfig = list(
        temperature = temperature,
        maxOutputTokens = max_output_tokens
      )
    )
  )
  
  if(response$status_code>200) {
    stop(paste("Error - ", content(response)$error$message))
  }
  
  candidates <- content(response)$candidates
  outputs <- unlist(lapply(candidates, function(candidate) candidate$content$parts))
  
  return(gsub("\n$", "", outputs))
  
}

# Test the function
text_input_1 <- "omg dis is da best day evr!!!"
cat("omg dis is da best day evr!!!\n➡️", gemini_text_normalization(text_input_1))

text_input_2 <- "yayyy, tysm this is sooo gr8!!! i cant beleive their is only twwo dayz left!!!!!"
cat("\nyayyy, tysm this is sooo gr8!!! i cant beleive their is only twwo dayz left!!!!!\n➡️", gemini_text_normalization(text_input_2))
```

## Function for Sentiment Analysis
1. Function: Read the comments or reviews, and analyze the sentiment of them (Positive, Neutral, Negative).
2. Use case: Preparing raw text data for analysis, especially in domains with inconsistent language (e.g., online reviews, customer feedback, social media comments).
```{r}
library(httr)
library(jsonlite)

# Function to call Google Gemini API for text cleaning
gemini_sentiment_analysis <- function(text_input,
                   temperature = 1,
                   max_output_tokens = 1024,
                   api_key = Sys.getenv("GEMINI_API_KEY"),
                   model = "gemini-2.0-flash") {  

  if (nchar(api_key) < 1) {
    api_key <- readline("Paste your API key here: ")
    Sys.setenv(GEMINI_API_KEY = api_key)
  }

  model_query <- paste0(model, ":generateContent")

  # Prompt for Text Normalization
  prompt <- paste0(
    "TASK: Analyze the sentiment of the given text and classify it as **Positive, Neutral, or Negative**.

    INSTRUCTIONS:
    - Identify the overall sentiment:
      - **Positive**: The text expresses enjoyment, excitement, satisfaction, or appreciation.
      - **Neutral**: The text states facts without emotion or shows a mix of positive and negative opinions.
      - **Negative**: The text expresses frustration, dissatisfaction, or disappointment.
    - If a statement lists **favorite things** (e.g., My favorite foods are...), assume **Positive** sentiment unless negative language is present.
    - The output should be **only the sentiment label**: `Positive`, `Neutral`, or `Negative`.
    
    EXAMPLES:
    
    1. **Input:** I love pizza, sushi, and burgers!
       **Output:** Positive
    
    2. **Input:** My top 3 movies are Inception, The Matrix, and Interstellar.
       **Output:** Positive (Because it lists `top` choices)
    
    3. **Input:** Movies I watched last month: Inception, Titanic, The Dark Knight.
       **Output:** Neutral (Just listing without opinion)
    
    4. **Input:** I hate horror movies. They're terrible.
       **Output:** Negative
    
    ===== INPUT TEXT BELOW =====

    ", text_input
  )

  response <- POST(
    url = paste0("https://generativelanguage.googleapis.com/v1beta/models/", model_query),
    query = list(key = api_key),
    content_type_json(),
    encode = "json",
    body = list(
      contents = list(
        parts = list(
          list(text = prompt) # prompt integrates user input
        )),
      generationConfig = list(
        temperature = temperature,
        maxOutputTokens = max_output_tokens
      )
    )
  )
  
  if(response$status_code>200) {
    stop(paste("Error - ", content(response)$error$message))
  }
  
  candidates <- content(response)$candidates
  outputs <- unlist(lapply(candidates, function(candidate) candidate$content$parts))
  
  return(gsub("\n$", "", outputs))
  
}

# Test the function
text_input_1 <- "im getting on borderlands and i will murder you all ,"
cat("im getting on borderlands and i will murder you all\n➡️", gemini_sentiment_analysis(text_input_1))

text_input_2 <- "Rock-Hard La Varlope, RARE & POWERFUL, HANDSOME JACKPOT, Borderlands 3 (Xbox) dlvr.it/RMTrgF  "
cat("\nRock-Hard La Varlope, RARE & POWERFUL, HANDSOME JACKPOT, Borderlands 3 (Xbox) dlvr.it/RMTrgF  \n➡️", gemini_sentiment_analysis(text_input_2))

text_input_3 <- "My 4 fave games are Minecraft. Borderlands 2.Forza horizon 4. Lego star wars"
cat("\nMy 4 fave games are Minecraft. Borderlands 2.Forza horizon 4. Lego star wars\n➡️", gemini_sentiment_analysis(text_input_2))

```

# read dataset
```{r}
twitter <- read.csv("data/twitter_training.csv")
head(twitter)
```


```{r}
library(dplyr)

# Function to rate-limit API calls (max 10 per minute)
rate_limited_sapply <- function(x, func) {
  results <- vector("list", length(x))  # Store results
  for (i in seq_along(x)) {
    results[[i]] <- func(x[i])  # Call the API function
    if (i %% 10 == 0) {  # After every 10 requests
      message("Rate limiting: Pausing for 60 seconds...")
      Sys.sleep(60)  # Pause for 1 minute
    } else {
      Sys.sleep(6)  # Delay each request by 6 seconds
    }
  }
  return(unlist(results))
}

# Read dataset
twitter <- read.csv("data/twitter_training.csv")
twitter_test <- head(twitter, 20)  # Select 20 rows for testing

# Apply remove noise with rate-limiting
twitter_test$Tweet.content_new <- rate_limited_sapply(twitter_test$Tweet.content, gemini_remove_noise)

# Apply text normalization with rate-limiting
twitter_test$Tweet.content_new <- rate_limited_sapply(twitter_test$Tweet.content_new, gemini_text_normalization)

# Apply sentiment analysis with rate-limiting
twitter_test$sentiment_new <- rate_limited_sapply(twitter_test$Tweet.content_new, gemini_sentiment_analysis)


```

```{r}
# Show results
twitter_test
```

